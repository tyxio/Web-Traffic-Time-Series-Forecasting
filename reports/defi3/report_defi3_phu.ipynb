{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This notebook is best viewed in [Google Colab](https://colab.research.google.com/drive/1rb5H7tVacoIHxjQix8yrOL21b17t7Vq8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dH-Nk05eehVY"
      },
      "source": [
        "## Time Series Forecasting with Neural Networks\r\n",
        "\r\n",
        "We have built different types of NN models for time series forecasting covering forecasts for a single time step as well as for multiple steps. Our study includes linear, dense, CNN, RNN models, as well as a convolutional se2seq neural network modeled after WaveNet. \r\n",
        "\r\n",
        "As our main objective was to learn more on how to build these networks using Tensoflow/Keras and how to transform the time series dataset to train and predict, we have decided to fuly develop these models from scratch without using the code made available to us during the exercise sessions. The code of all our models is available on [github](https://github.com/tyxio/Web-Traffic-Time-Series-Forecasting) in the folder [src_neural_network](https://github.com/tyxio/Web-Traffic-Time-Series-Forecasting/tree/main/src_neural_network). \r\n",
        "\r\n",
        "Our work is based on these two excellent publications:\r\n",
        " \r\n",
        "*   [TensorFlow time series forecasting tutorial](https://www.tensorflow.org/tutorials/structured_data/time_series)\r\n",
        "*   [JEddy32's TimeSeries_Seq2Seq](https://github.com/JEddy92/TimeSeries_Seq2Seq)\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2xkj2RnqkvP"
      },
      "source": [
        "### Simple multi steps models\r\n",
        "We have built several simple models to make multiple time step predictions. These models make \"single shot predictions\" where the entire period is predicted at once (i.e. 21 days). These models predict also all features (series) at once. The code is avalaible in the class [MultiStepModels](https://github.com/tyxio/Web-Traffic-Time-Series-Forecasting/blob/main/src_neural_network/multiStepModels/MultiStepModels.py). As a example, here are the Dense and CNN models:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E__yEvJcsqYL"
      },
      "outputs": [],
      "source": [
        "def model_dense(self):\r\n",
        "    multi_dense_model = tf.keras.Sequential([\r\n",
        "        # Take the last time step.\r\n",
        "        # Shape [batch, time, features] => [batch, 1, features]\r\n",
        "        tf.keras.layers.Lambda(lambda x: x[:, -1:, :]),\r\n",
        "        # Shape => [batch, 1, dense_units]\r\n",
        "        tf.keras.layers.Dense(1024, activation='relu'),\r\n",
        "        tf.keras.layers.Dense(1024, activation='relu'),\r\n",
        "        tf.keras.layers.Dense(1024, activation='relu'),\r\n",
        "        # Shape => [batch, out_steps*features]\r\n",
        "        tf.keras.layers.Dense(MS_OUT_STEPS*self.num_features,\r\n",
        "                            kernel_initializer=tf.initializers.zeros()),\r\n",
        "        # Shape => [batch, out_steps, features]\r\n",
        "        tf.keras.layers.Reshape([MS_OUT_STEPS, self.num_features])\r\n",
        "    ])\r\n",
        "\r\n",
        "    history = compile_and_fit(multi_dense_model, self.multi_window)\r\n",
        "\r\n",
        "def model_cnn(self):\r\n",
        "    CONV_WIDTH = 3\r\n",
        "    multi_conv_model = tf.keras.Sequential([\r\n",
        "        # Shape [batch, time, features] => [batch, CONV_WIDTH, features]\r\n",
        "        tf.keras.layers.Lambda(lambda x: x[:, -CONV_WIDTH:, :]),\r\n",
        "        # Shape => [batch, 1, conv_units]\r\n",
        "        tf.keras.layers.Conv1D(256, activation='relu', kernel_size=(CONV_WIDTH)),\r\n",
        "        # Shape => [batch, 1,  out_steps*features]\r\n",
        "        tf.keras.layers.Dense(MS_OUT_STEPS*self.num_features,\r\n",
        "                            kernel_initializer=tf.initializers.zeros()),\r\n",
        "        # Shape => [batch, out_steps, features]\r\n",
        "        tf.keras.layers.Reshape([MS_OUT_STEPS, self.num_features])\r\n",
        "    ])\r\n",
        "\r\n",
        "    history = compile_and_fit(multi_conv_model, self.multi_window)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JI_P7ryMu6S8"
      },
      "source": [
        "The plots below show the predictions over the course of 21 days for a few arbitrary series. The green dots show the target prediction values (labels), the orange dots shows the actual prediction:\r\n",
        "\r\n",
        "| Dense Model | CNN Model |\r\n",
        "|---|---|\r\n",
        "|![title](https://drive.google.com/uc?export=view&id=1t7En_1lkawq5wLvyVuMnwdT0o7r4Kafz)|![title](https://drive.google.com/uc?export=view&id=1b4r63S_3LKsDKT5Frp0DvzhGAOZCwpEk)\r\n",
        "\r\n",
        "These models look to do a good job picking up on seasonality and trend, and handling the prediction horizon for many series. But the Kaggle scores are disapointing: around 17.5. So, we decided to focus on a more promising network: an autoencoder model using DeepMind's WaveNet concepts!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i1Fhz7sWkKj0"
      },
      "source": [
        "### Forecasting with a convolutional sequence-to-sequence neural network modeled after WaveNet\r\n",
        "\r\n",
        "The most promising network that we have modeled is a convolutional Seq2Seq neural network using DeepMind's WaveNet model architecture. This work is based on several readings of articles, in particular [J.Eddy's blog](https://jeddy92.github.io/JEddy92.github.io/ts_seq2seq_conv/)\r\n",
        "with its [accompaying notebooks](https://github.com/JEddy92/TimeSeries_Seq2Seq). \r\n",
        "Using the ideas and code developed by J. Eddy, we have trained a Wavenet-style network with a stack of 2 x 9 dilated causal convolution layers followed by 2 dense layers. Using 9 dilated convolution layers allows to capture over a year of history with a daily time series.\r\n",
        "\r\n",
        "Here's the [code](https://github.com/tyxio/Web-Traffic-Time-Series-Forecasting/blob/main/src_neural_network/multiStepModels/Seq2SeqConvFull.py) defining the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DscclS8u2xBm"
      },
      "outputs": [],
      "source": [
        "def build_training_model(self):\r\n",
        "        \r\n",
        "        # convolutional operation parameters\r\n",
        "        n_filters = S2S_CONVFULL_N_FILTERS # 32 \r\n",
        "        filter_width = S2S_CONVFULL_FILTER_WIDTH # 2\r\n",
        "        dilation_rates = [2**i for i in range(S2S_CONVFULL_N_DILATIONS)] * 2 # 9\r\n",
        "        n_dilation_layers = len(dilation_rates)\r\n",
        "        n_dilation_nodes = 2**(S2S_CONVFULL_N_DILATIONS-1)\r\n",
        "\r\n",
        "        # define an input history series and pass it through a stack of dilated causal convolution blocks. \r\n",
        "        history_seq = Input(shape=(None, 1))\r\n",
        "        x = history_seq\r\n",
        "\r\n",
        "        skips = []\r\n",
        "        for dilation_rate in dilation_rates:        \r\n",
        "            # preprocessing - equivalent to time-distributed dense\r\n",
        "            x = Conv1D(n_dilation_layers, 1, padding='same', activation='relu')(x) \r\n",
        "            \r\n",
        "            # filter convolution\r\n",
        "            x_f = Conv1D(filters=n_filters,\r\n",
        "                        kernel_size=filter_width, \r\n",
        "                        padding='causal',\r\n",
        "                        dilation_rate=dilation_rate)(x)\r\n",
        "            \r\n",
        "            # gating convolution\r\n",
        "            x_g = Conv1D(filters=n_filters,\r\n",
        "                        kernel_size=filter_width, \r\n",
        "                        padding='causal',\r\n",
        "                        dilation_rate=dilation_rate)(x)\r\n",
        "            \r\n",
        "            # multiply filter and gating branches\r\n",
        "            z = Multiply()([Activation('tanh')(x_f),\r\n",
        "                            Activation('sigmoid')(x_g)])\r\n",
        "            \r\n",
        "            # postprocessing - equivalent to time-distributed dense\r\n",
        "            z = Conv1D(n_dilation_layers, 1, padding='same', activation='relu')(z)\r\n",
        "            \r\n",
        "            # residual connection\r\n",
        "            x = Add()([x, z])    \r\n",
        "            \r\n",
        "            # collect skip connections\r\n",
        "            skips.append(z)\r\n",
        "\r\n",
        "        # add all skip connection outputs \r\n",
        "        out = Activation('relu')(Add()(skips))\r\n",
        "\r\n",
        "        # final time-distributed dense layers \r\n",
        "        out = Conv1D(n_dilation_nodes, 1, padding='same')(out)\r\n",
        "        out = Activation('relu')(out)\r\n",
        "        out = Dropout(.2)(out)\r\n",
        "        out = Conv1D(1, 1, padding='same')(out)\r\n",
        "\r\n",
        "        pred_seq_train = Lambda(self.slice, arguments={'seq_length':HORIZON})(out)\r\n",
        "\r\n",
        "        model = Model(history_seq, pred_seq_train)\r\n",
        "        model.compile(Adam(), loss='mean_absolute_error')\r\n",
        "\r\n",
        "        print(model.summary())\r\n",
        "\r\n",
        "        return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Euoq28rdN9Ox"
      },
      "source": [
        "Before training this model, we have applied 2 transformations to the data:\r\n",
        "\r\n",
        "\r\n",
        "1.   Removed the outliners using a Hample filter with window_size=8, threshold=3\r\n",
        "2.   Applied a log1p transformation to smooth out the scale of traffic accross different series, and then centering the series around the mean of their training dataset.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2qtEW8PkKj8"
      },
      "source": [
        "This model trains quickly. The plot below shows the training convergence. We stopped training after 150 epochs.\r\n",
        "\r\n",
        "![title](https://drive.google.com/uc?export=view&id=1vwp7KQO1YZQk2f3dIGvC8qcl6VnXHru9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-c1O_6kHkKj8"
      },
      "source": [
        "We have estimated the SMAPE value for each series with a 3-week prediction at the end of the training period (2017-07-31 to 2017-08-20):\r\n",
        "\r\n",
        "![title](https://drive.google.com/uc?export=view&id=173nEsKjM9W2sNPwOjcoZLTEgLeFwWIgD)\r\n",
        "\r\n",
        "The model does a good job picking up on seasonality and trend, and handling the prediction horizon for many series (SMAPE < 10). However, there is a significant number of series that are not properly modeled (e.g. series 19, 20, 40-46). The plots below give a few examples of predictions, as well as forecasting over the period 2017-08-21 to 2017-09-10. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Hu9gy2ckKj9"
      },
      "source": [
        "\r\n",
        "\r\n",
        "\r\n",
        "|SMAPE             | Predictions |Forecasts|\r\n",
        "|:---              |     :----:  |    :----:   |\r\n",
        "|series-15 smape=7.4|![title](https://drive.google.com/uc?export=view&id=1frWHAHe61X7LpfYwnytRhal8p9JczMvb)|![title](https://drive.google.com/uc?export=view&id=1uooaHsAQruNSySfDYkhvYep1asZgsiw7)|\r\n",
        "|series-21 smape=12.5|![title](https://drive.google.com/uc?export=view&id=14TevHFga8UxYa63_kKq8LzecEesC3vkz)|![title](https://drive.google.com/uc?export=view&id=1eZR8YTXn8gbkkApxx-b0CFB-zZZSMmnO)|\r\n",
        "|series-69 smape=3.95|![title](https://drive.google.com/uc?export=view&id=1paO7od9sHiah55tzYT7wkIhjPRVWEGZj)|![title](https://drive.google.com/uc?export=view&id=1BGbxhwTNoIqTqYqUpTt911T_RH40uNis)|\r\n",
        "|series-19 smape=70.6|![title](https://drive.google.com/uc?export=view&id=19k_rYhn7EN9kFAVJiHbDSZlNfUvWywHB)|![title](https://drive.google.com/uc?export=view&id=1FL5oJPGCP9S_YzYYqOJ8RhSgGaeYNlXl)|\r\n",
        "|series-46 smape=29.2|![title](https://drive.google.com/uc?export=view&id=1MUh-mH4c84EsdD8haAEHKbzNqe8TVh8s)|![title](https://drive.google.com/uc?export=view&id=1KOfYsUI6RYqyt1fDRfrrPlH2493LCMlS)|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7g7_sxja0Ee"
      },
      "source": [
        "It is worth mentioning that the main difficulties in developing this solution were in the steps before and after building and training the tensorflow model. Namely:\r\n",
        "\r\n",
        "\r\n",
        "*   Formatting the data for modeling: the time series must be first partitioned appropriately into encoding and decoding intervals; then additional transformation steps are required to extract the data into arrays that can be passed to the keras model's input layer. There is a nice explaination on how the data must be split and transformed in this [blog](https://github.com/Arturus/kaggle-web-traffic/blob/master/how_it_works.md#training-and-validation). We took the code from [Eddy's blog](https://github.com/JEddy92/TimeSeries_Seq2Seq/blob/master/notebooks/TS_Seq2Seq_Intro.ipynb). \r\n",
        "*   Prepare for inference and forecast: many articles on autoencoders ignore this step. They explain how to build a model and train it but say nothing or very little on how to use the trained model for inference. Again, Eddy's blog was of a great help to define an [inference architecture](https://github.com/JEddy92/TimeSeries_Seq2Seq/blob/master/notebooks/TS_Seq2Seq_Intro.ipynb) to feed the encoder and then have the decoder generates a prediction for each time step. Something to note: we were not able to save the trained model and load it for inference in a seperate module. Tensorflow/Keras stopped with errors when loading the saved that we could not address. So, training and inference were done in sequences (which is not ideal of course). \r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUQl7JZ2gdda"
      },
      "source": [
        "#### Kaggle Competition\r\n",
        "\r\n",
        "This model does not score well: 18.05. Not sure I understand why...\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJITMAAZ4iAb"
      },
      "source": [
        "### Summing Up\r\n",
        "\r\n",
        "Well, the model's performance did not turn out the way we might have expected. The Kaggle score is not that good. The figures above indicate that our model can understand certain patterns but fail to capture the details of the variability (e.g. series-46). Abrut changes just before the forcast period are clearly not well taken into account (e.g. series-19). However, there are a number of reasons to consider it is possible to improve the results:\r\n",
        "\r\n",
        "\r\n",
        "*   We did not tune hyperparameters like dropout, loss, optimizer... It seems that the winner of the original Kaggle competition to predict Wikipedia web traffic did a lot of smart hyperparameter searches (in A. Nielsen's Pratical Time Series Analysis, O'Reilly).\r\n",
        "*   We did not try different encoder-decoder architectures. We could play with the number of dilation layers or the number of filters units. \r\n",
        "*   Maybe we did not explore the data enough and did not applied the best transformation before submitting the data to the model for training and inference. \r\n",
        "\r\n",
        "Clearly, deep learning for time series forecasting is not a magic bullet.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgYhqgM2BS4W"
      },
      "source": [
        "### References\r\n",
        "\r\n",
        "*  TensorFlow time series forecasting tutorial: https://www.tensorflow.org/tutorials/structured_data/time_series\r\n",
        "*  JEddy32 TimeSeries_Seq2Seq Github: https://github.com/JEddy92/TimeSeries_Seq2Seq\r\n",
        "*  Philippe Huet's defi3 github: https://github.com/tyxio/Web-Traffic-Time-Series-Forecasting\r\n",
        "\r\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "report-defi3-phu.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "nbconvert_exporter": "python",
      "version": "3.8.6-final"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 0
}